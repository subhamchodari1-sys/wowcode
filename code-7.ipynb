{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1743a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple & Fast Transformer Fine-tuning - No BS Version\n",
    "# Just the essentials to get it working quickly!\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Traine\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "print(\"■ Quick Fine-tuning Setup - Let's go fast!\")\n",
    "# Use DistilBERT - it's much faster than BERT\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "MAX_LENGTH = 128 # Short sequences = faster training\n",
    "BATCH_SIZE = 32 # Bigger batches = faster on GPU\n",
    "EPOCHS = 1 # Just 1 epoch to see if it works\n",
    "print(f\"Using model: {MODEL_NAME}\")\n",
    "print(f\"Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "# Load tiny dataset (just 1000 samples)\n",
    "print(\"■ Loading AG News dataset (small subset)...\")\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "small_train = dataset[\"train\"].select(range(1000)) # Only 1k samples\n",
    "small_test = dataset[\"test\"].select(range(200)) # Only 200 test samples\n",
    "print(f\"Train samples: {len(small_train)}\")\n",
    "print(f\"Test samples: {len(small_test)}\")\n",
    "# Quick tokenizer setup\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "def tokenize_function(examples):\n",
    "return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
    "# Tokenize datasets\n",
    "print(\"■ Quick tokenization...\")\n",
    "train_dataset = small_train.map(tokenize_function, batched=True)\n",
    "eval_dataset = small_test.map(tokenize_function, batched=True)\n",
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=4)\n",
    "# Simple metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "predictions, labels = eval_pred\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "return {\"accuracy\": accuracy_score(labels, predictions)}\n",
    "# Minimal training arguments - optimized for speed\n",
    "training_args = TrainingArguments(\n",
    "output_dir=\"./quick_results\",\n",
    "num_train_epochs=EPOCHS,\n",
    "per_device_train_batch_size=BATCH_SIZE,\n",
    "per_device_eval_batch_size=BATCH_SIZE,\n",
    "logging_steps=50,\n",
    "eval_strategy=\"epoch\",\n",
    "save_strategy=\"no\", # Don't save to disk - saves time\n",
    "report_to=None, # No logging to external services\n",
    "dataloader_num_workers=0, # Avoid multiprocessing overhead\n",
    ")\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "model=model,\n",
    "args=training_args,\n",
    "train_dataset=train_dataset,\n",
    "eval_dataset=eval_dataset,\n",
    "compute_metrics=compute_metrics,\n",
    ")\n",
    "print(\"■ Starting quick training...\")\n",
    "print(\"=\" * 50)\n",
    "# Train the model\n",
    "trainer.train()\n",
    "# Quick evaluationprint(\"\\n■ Quick evaluation...\")\n",
    "results = trainer.evaluate()\n",
    "print(f\"Final Accuracy: {results['eval_accuracy']:.4f}\")\n",
    "# Test some predictions\n",
    "print(\"\\n■ Testing predictions...\")\n",
    "test_texts = [\n",
    "\"Apple stock rises after earnings report\",\n",
    "\"Football team wins championship\",\n",
    "\"New AI breakthrough announced\",\n",
    "\"Political debate continues\"\n",
    "]\n",
    "label_names = [\"World\", \"Sports\", \"Business\", \"Technology\"]\n",
    "# Get device that model is on\n",
    "device = next(model.parameters()).device\n",
    "print(f\"Model is on: {device}\")\n",
    "for text in test_texts:\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_LENGT\n",
    "# Move inputs to same device as model\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "with torch.no_grad():\n",
    "outputs = model(**inputs)\n",
    "prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
    "print(f\"Text: '{text[:40]}...'\")\n",
    "print(f\"Predicted: {label_names[prediction]}\\n\")\n",
    "print(\"■ Done! That was quick!\")\n",
    "print(\"\\nTo make it even faster next time:\")\n",
    "print(\"- Use fewer samples: dataset.select(range(500))\")\n",
    "print(\"- Shorter sequences: MAX_LENGTH = 64\")\n",
    "print(\"- Bigger batches: BATCH_SIZE = 64 (if GPU memory allows)\")\n",
    "print(\"- Use 'distilbert-base-uncased' (smallest model)\")\n",
    "# Optional: Save just the model weights if needed\n",
    "# model.save_pretrained(\"./quick_model\")\n",
    "# tokenizer.save_pretrained(\"./quick_model\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
