{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7eb3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration class for hyperparameters and settings\"\"\"\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE = 0.001\n",
    "    EPOCHS = 10\n",
    "    NUM_CLASSES = 10\n",
    "    INPUT_SIZE = 28 * 28  # MNIST image size\n",
    "    HIDDEN_SIZE_1 = 512\n",
    "    HIDDEN_SIZE_2 = 256\n",
    "    DROPOUT_RATE = 0.5\n",
    "\n",
    "    # CNN specific\n",
    "    CNN_CHANNELS_1 = 32\n",
    "    CNN_CHANNELS_2 = 64\n",
    "    CNN_KERNEL_SIZE = 3\n",
    "    POOL_SIZE = 2\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"Get the best available device (GPU if available, else CPU)\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "def load_mnist_data():\n",
    "    \"\"\"\n",
    "    Load and preprocess MNIST dataset\n",
    "    Returns train and test data loaders\n",
    "    \"\"\"\n",
    "    # Data preprocessing pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # Convert PIL Image to tensor and normalize to [0,1]\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # MNIST specific normalization\n",
    "    ])\n",
    "\n",
    "    # Download and load training data\n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        root='./data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    # Download and load test data\n",
    "    test_dataset = torchvision.datasets.MNIST(\n",
    "        root='./data',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=Config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=Config.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    print(f\"Batch size: {Config.BATCH_SIZE}\")\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "class ANN(nn.Module):\n",
    "    \"\"\"\n",
    "    Artificial Neural Network (Fully Connected Network)\n",
    "    Architecture: Input -> FC1 -> ReLU -> Dropout -> FC2 -> ReLU -> Dropout -> FC3 -> Output\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ANN, self).__init__()\n",
    "\n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(Config.INPUT_SIZE, Config.HIDDEN_SIZE_1)\n",
    "        self.fc2 = nn.Linear(Config.HIDDEN_SIZE_1, Config.HIDDEN_SIZE_2)\n",
    "        self.fc3 = nn.Linear(Config.HIDDEN_SIZE_2, Config.NUM_CLASSES)\n",
    "\n",
    "        # Dropout layers for regularization\n",
    "        self.dropout = nn.Dropout(Config.DROPOUT_RATE)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize network weights using Xavier/Glorot initialization\"\"\"\n",
    "        for layer in [self.fc1, self.fc2, self.fc3]:\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward propagation\"\"\"\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network\n",
    "    Architecture: Conv1 -> ReLU -> MaxPool -> Conv2 -> ReLU -> MaxPool -> Flatten -> FC1 -> ReLU -> Dropout -> FC2\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, Config.CNN_CHANNELS_1, Config.CNN_KERNEL_SIZE, padding=1)\n",
    "        self.conv2 = nn.Conv2d(Config.CNN_CHANNELS_1, Config.CNN_CHANNELS_2, Config.CNN_KERNEL_SIZE, padding=1)\n",
    "        self.pool = nn.MaxPool2d(Config.POOL_SIZE, Config.POOL_SIZE)\n",
    "\n",
    "        self.fc_input_size = Config.CNN_CHANNELS_2 * 7 * 7\n",
    "        self.fc1 = nn.Linear(self.fc_input_size, Config.HIDDEN_SIZE_2)\n",
    "        self.fc2 = nn.Linear(Config.HIDDEN_SIZE_2, Config.NUM_CLASSES)\n",
    "        self.dropout = nn.Dropout(Config.DROPOUT_RATE)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class NetworkTrainer:\n",
    "    \"\"\"Training and evaluation handler\"\"\"\n",
    "\n",
    "    def __init__(self, model, device, train_loader, test_loader):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=Config.LEARNING_RATE)\n",
    "        self.train_losses, self.train_accuracies = [], []\n",
    "        self.test_losses, self.test_accuracies = [], []\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        for data, targets in tqdm(self.train_loader, desc='Training'):\n",
    "            data, targets = data.to(self.device), targets.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(data)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "        return running_loss / len(self.train_loader), 100. * correct / total\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        loss, correct, total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for data, targets in self.test_loader:\n",
    "                data, targets = data.to(self.device), targets.to(self.device)\n",
    "                outputs = self.model(data)\n",
    "                loss += self.criterion(outputs, targets).item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "        return loss / len(self.test_loader), 100. * correct / total\n",
    "\n",
    "    def train(self, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_acc = self.train_epoch()\n",
    "            test_loss, test_acc = self.evaluate()\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.train_accuracies.append(train_acc)\n",
    "            self.test_losses.append(test_loss)\n",
    "            self.test_accuracies.append(test_acc)\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Acc: {train_acc:.2f}%, Test Acc: {test_acc:.2f}%\")\n",
    "        return self.train_losses, self.train_accuracies, self.test_losses, self.test_accuracies\n",
    "\n",
    "def main():\n",
    "    device = get_device()\n",
    "    train_loader, test_loader = load_mnist_data()\n",
    "    ann_model = ANN()\n",
    "    cnn_model = CNN()\n",
    "    ann_trainer = NetworkTrainer(ann_model, device, train_loader, test_loader)\n",
    "    cnn_trainer = NetworkTrainer(cnn_model, device, train_loader, test_loader)\n",
    "    ann_history = ann_trainer.train(Config.EPOCHS)\n",
    "    cnn_history = cnn_trainer.train(Config.EPOCHS)\n",
    "    print(\"ANN Final Accuracy:\", ann_history[3][-1])\n",
    "    print(\"CNN Final Accuracy:\", cnn_history[3][-1])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
