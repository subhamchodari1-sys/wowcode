{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a56ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import math\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "# Hyperparameters\n",
    "input_size = 15 # Expanded vocabulary\n",
    "output_size = 15\n",
    "hidden_size = 256 # Reduced for better generalization with small dataset\n",
    "embedding_dim = 128\n",
    "num_layers = 2\n",
    "learning_rate = 0.003 # Increased for faster learning\n",
    "num_epochs = 500 # More epochs needed\n",
    "batch_size = 1\n",
    "dropout = 0.2\n",
    "lstm_dropout = 0.2\n",
    "beam_width = 3\n",
    "top_k = 3\n",
    "max_len = 12\n",
    "teacher_forcing_ratio = 0.8 # Higher for better stability\n",
    "# Corrected and expanded vocabulary\n",
    "word_to_idx_en = {\n",
    "'<SOS>': 0, '<EOS>': 1, '<PAD>': 2, 'hello': 3, 'world': 4, 'i': 5, 'am': 6,\n",
    "'good': 7, 'how': 8, 'are': 9, 'you': 10, 'my': 11, 'name': 12, 'is': 13, 'fine': 14\n",
    "}\n",
    "word_to_idx_fr = {\n",
    "'<SOS>': 0, '<EOS>': 1, '<PAD>': 2, 'bonjour': 3, 'monde': 4, 'je': 5, 'suis': 6,\n",
    "'bien': 7, 'comment': 8, 'allez': 9, 'vous': 10, 'mon': 11, 'nom': 12, 'est': 13, 'ca': 14\n",
    "}\n",
    "idx_to_word_en = {v: k for k, v in word_to_idx_en.items()}\n",
    "idx_to_word_fr = {v: k for k, v in word_to_idx_fr.items()}\n",
    "# Corrected dataset with proper English-French translations\n",
    "data = [\n",
    "# English -> French\n",
    "([3, 4, 1], [3, 4, 1]), # hello world -> bonjour monde\n",
    "([5, 6, 7, 1], [5, 6, 7, 1]), # i am good -> je suis bien\n",
    "([8, 9, 10, 1], [8, 9, 10, 1]), # how are you -> comment allez vous\n",
    "([11, 12, 13, 1], [11, 12, 13, 1]), # my name is -> mon nom est\n",
    "([5, 6, 14, 1], [5, 6, 7, 1]), # i am fine -> je suis bien\n",
    "([3, 1], [3, 1]), # hello -> bonjour\n",
    "([8, 9, 10, 14, 1], [8, 14, 9, 10, 1]), # how are you fine -> comment ca allez vous\n",
    "([5, 6, 1], [5, 6, 1]), # i am -> je suis\n",
    "]\n",
    "# Convert to proper tensors\n",
    "data = [(torch.tensor(src, dtype=torch.long), torch.tensor(tgt, dtype=torch.long)) for src, tgt in data]\n",
    "# Improved Attention mechanism\n",
    "class Attention(nn.Module):\n",
    "\tdef __init__(self, hidden_size):\n",
    "\t\tsuper(Attention, self).__init__()\n",
    "\t\tself.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "\t\tself.v = nn.Parameter(torch.rand(hidden_size))\n",
    "\t\tstdv = 1. / math.sqrt(self.v.size(0))\n",
    "\t\tself.v.data.uniform_(-stdv, stdv)\n",
    "\n",
    "\tdef forward(self, hidden, encoder_outputs):\n",
    "\t\tseq_len = encoder_outputs.size(1)\n",
    "\t\tbatch_size = encoder_outputs.size(0)\n",
    "\t\t# Use the last layer's hidden state\n",
    "\t\thidden = hidden[-1].unsqueeze(1).repeat(1, seq_len, 1)  # [batch, seq_len, hidden]\n",
    "\t\t# Calculate energy\n",
    "\t\tenergy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "\t\tattention = torch.sum(self.v * energy, dim=2)\n",
    "\t\treturn F.softmax(attention, dim=1)\n",
    "\n",
    "# Enhanced Encoder\n",
    "class EncoderLSTM(nn.Module):\n",
    "\tdef __init__(self, input_size, embedding_dim, hidden_size, num_layers, dropout, lstm_dropout):\n",
    "\t\tsuper(EncoderLSTM, self).__init__()\n",
    "\t\tself.embedding = nn.Embedding(input_size, embedding_dim)\n",
    "\t\tself.lstm = nn.LSTM(\n",
    "\t\t\tembedding_dim, hidden_size, num_layers,\n",
    "\t\t\tdropout=lstm_dropout if num_layers > 1 else 0,\n",
    "\t\t\tbatch_first=True, bidirectional=False\n",
    "\t\t)\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\tdef forward(self, src):\n",
    "\t\tembedded = self.dropout(self.embedding(src))\n",
    "\t\toutputs, (hidden, cell) = self.lstm(embedded)\n",
    "\t\treturn outputs, hidden, cell\n",
    "\n",
    "# Enhanced Decoder with Attention\n",
    "class DecoderLSTM(nn.Module):\n",
    "\tdef __init__(self, output_size, embedding_dim, hidden_size, num_layers, dropout, lstm_dropout):\n",
    "\t\tsuper(DecoderLSTM, self).__init__()\n",
    "\t\tself.output_size = output_size\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.num_layers = num_layers\n",
    "\t\tself.embedding = nn.Embedding(output_size, embedding_dim)\n",
    "\t\tself.attention = Attention(hidden_size)\n",
    "\t\tself.lstm = nn.LSTM(\n",
    "\t\t\tembedding_dim + hidden_size, hidden_size, num_layers,\n",
    "\t\t\tdropout=lstm_dropout if num_layers > 1 else 0,\n",
    "\t\t\tbatch_first=True\n",
    "\t\t)\n",
    "\t\tself.fc = nn.Linear(hidden_size * 2, output_size)  # Concat context and hidden\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\tdef forward(self, tgt, hidden, cell, encoder_outputs):\n",
    "\t\t# tgt shape: [batch_size]\n",
    "\t\ttgt = tgt.unsqueeze(1)  # [batch_size, 1]\n",
    "\t\tembedded = self.dropout(self.embedding(tgt))  # [batch_size, 1, embedding_dim]\n",
    "\t\t# Calculate attention\n",
    "\t\tattn_weights = self.attention(hidden, encoder_outputs)  # [batch_size, seq_len]\n",
    "\t\tcontext = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)  # [batch_size, 1, hidden_size]\n",
    "\t\t# Combine embedding and context\n",
    "\t\tlstm_input = torch.cat((embedded, context), dim=2)  # [batch_size, 1, embedding_dim + hidden_size]\n",
    "\t\t# LSTM forward pass\n",
    "\t\toutput, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "\t\t# Combine output and context for final prediction\n",
    "\t\tcombined_output = torch.cat((output.squeeze(1), context.squeeze(1)), dim=1)\n",
    "\t\tprediction = self.fc(combined_output)\n",
    "\t\treturn prediction, hidden, cell, attn_weights\n",
    "\n",
    "# Seq2Seq Model\n",
    "class Seq2Seq(nn.Module):\n",
    "\tdef __init__(self, encoder, decoder):\n",
    "\t\tsuper(Seq2Seq, self).__init__()\n",
    "\t\tself.encoder = encoder\n",
    "\t\tself.decoder = decoder\n",
    "\n",
    "\tdef forward(self, src, tgt, teacher_forcing_ratio=0.8):\n",
    "\t\tbatch_size = src.size(0)\n",
    "\t\ttgt_len = tgt.size(1)\n",
    "\t\ttgt_vocab_size = self.decoder.output_size\n",
    "\t\toutputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(src.device)\n",
    "\t\tencoder_outputs, hidden, cell = self.encoder(src)\n",
    "\t\tinput = tgt[:, 0]  # Start token\n",
    "\t\tfor t in range(1, tgt_len):\n",
    "\t\t\tprediction, hidden, cell, _ = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "\t\t\toutputs[:, t] = prediction\n",
    "\t\t\tteacher_force = random.random() < teacher_forcing_ratio\n",
    "\t\t\ttop1 = prediction.argmax(1)\n",
    "\t\t\tinput = tgt[:, t] if teacher_force else top1\n",
    "\t\treturn outputs\n",
    "\n",
    "# Enhanced training function\n",
    "def train(model, data, optimizer, criterion, num_epochs, device):\n",
    "\tmodel.train()\n",
    "\tbest_loss = float('inf')\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\tepoch_loss = 0\n",
    "\t\trandom.shuffle(data)  # Shuffle data each epoch\n",
    "\t\tfor src, tgt in data:\n",
    "\t\t\tsrc = src.unsqueeze(0).to(device)\n",
    "\t\t\ttgt = tgt.unsqueeze(0).to(device)\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\toutput = model(src, tgt, teacher_forcing_ratio)\n",
    "\t\t\t# Reshape for loss calculation\n",
    "\t\t\toutput_dim = output.shape[-1]\n",
    "\t\t\toutput = output[:, 1:].contiguous().view(-1, output_dim)\n",
    "\t\t\ttgt = tgt[:, 1:].contiguous().view(-1)\n",
    "\t\t\tloss = criterion(output, tgt)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\t# Gradient clipping\n",
    "\t\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\tepoch_loss += loss.item()\n",
    "\t\tavg_loss = epoch_loss / len(data)\n",
    "\t\tif avg_loss < best_loss:\n",
    "\t\t\tbest_loss = avg_loss\n",
    "\t\tif (epoch + 1) % 50 == 0:\n",
    "\t\t\tprint(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Best Loss: {best_loss:.4f}')\n",
    "# Improved decoding strategies\n",
    "def greedy_decode(model, src, max_len, start_token=0, device='cpu'):\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "src = torch.tensor(src, dtype=torch.long).unsqueeze(0).to(device)\n",
    "encoder_outputs, hidden, cell = model.encoder(src)\n",
    "outputs = [start_token]\n",
    "input = torch.tensor([start_token], dtype=torch.long).to(device)\n",
    "for _ in range(max_len):\n",
    "prediction, hidden, cell, _ = model.decoder(input, hidden, cell, encoder_outputs)\n",
    "pred_token = prediction.argmax(1).item()\n",
    "if pred_token == 1: # EOS token\n",
    "break\n",
    "outputs.append(pred_token)\n",
    "input = torch.tensor([pred_token], dtype=torch.long).to(device)\n",
    "return [idx_to_word_fr[idx] for idx in outputs[1:] if idx in idx_to_word_fr]\n",
    "def beam_search_decode(model, src, beam_width, max_len, start_token=0, device='cpu'):\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "src = torch.tensor(src, dtype=torch.long).unsqueeze(0).to(device)\n",
    "encoder_outputs, hidden, cell = model.encoder(src)\n",
    "# Initialize beams\n",
    "beams = [([start_token], 0.0, hidden, cell)]\n",
    "for step in range(max_len):\n",
    "new_beams = []\n",
    "for seq, score, h, c in beams:\n",
    "if seq[-1] == 1: # EOS token\n",
    "new_beams.append((seq, score, h, c))\n",
    "continue\n",
    "input = torch.tensor([seq[-1]], dtype=torch.long).to(device)\n",
    "prediction, new_h, new_c, _ = model.decoder(input, h, c, encoder_outputs)\n",
    "log_probs = F.log_softmax(prediction, dim=1)\n",
    "top_log_probs, top_idx = log_probs.topk(beam_width)\n",
    "for lp, idx in zip(top_log_probs[0], top_idx[0]):\n",
    "new_seq = seq + [idx.item()]\n",
    "new_score = score + lp.item()\n",
    "new_beams.append((new_seq, new_score, new_h, new_c))\n",
    "# Keep top beams\n",
    "beams = sorted(new_beams, key=lambda x: x[1] / len(x[0]), reverse=True)[:beam_width]best_seq = beams[0][0][1:] # Remove start token\n",
    "return [idx_to_word_fr[idx] for idx in best_seq if idx != 1 and idx in idx_to_word_fr]\n",
    "def top_k_sampling_decode(model, src, top_k, max_len, start_token=0, device='cpu', temperature=0.8):\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "src = torch.tensor(src, dtype=torch.long).unsqueeze(0).to(device)\n",
    "encoder_outputs, hidden, cell = model.encoder(src)\n",
    "outputs = [start_token]\n",
    "input = torch.tensor([start_token], dtype=torch.long).to(device)\n",
    "for _ in range(max_len):\n",
    "prediction, hidden, cell, _ = model.decoder(input, hidden, cell, encoder_outputs)\n",
    "# Apply temperature\n",
    "scaled_logits = prediction / temperature\n",
    "probs = F.softmax(scaled_logits, dim=1)\n",
    "# Top-k sampling\n",
    "top_probs, top_idx = probs.topk(min(top_k, probs.size(1)))\n",
    "top_probs = top_probs / top_probs.sum(dim=1, keepdim=True)\n",
    "pred_token = top_idx[0][torch.multinomial(top_probs[0], 1).item()].item()\n",
    "if pred_token == 1: # EOS token\n",
    "break\n",
    "outputs.append(pred_token)\n",
    "input = torch.tensor([pred_token], dtype=torch.long).to(device)\n",
    "return [idx_to_word_fr[idx] for idx in outputs[1:] if idx in idx_to_word_fr]\n",
    "# BLEU score evaluation\n",
    "def evaluate_bleu(reference, candidate):\n",
    "if not candidate or not reference:\n",
    "return 0.0\n",
    "return sentence_bleu([reference], candidate, weights=(1, 0, 0, 0)) # 1-gram BLEU for small sequences\n",
    "# Main execution\n",
    "def main():\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "# Initialize model\n",
    "encoder = EncoderLSTM(input_size, embedding_dim, hidden_size, num_layers, dropout, lstm_dropout).to(device)\n",
    "decoder = DecoderLSTM(output_size, embedding_dim, hidden_size, num_layers, dropout, lstm_dropout).to(device)\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "# Initialize optimizer and criterion\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=2) # Ignore PAD token\n",
    "print(\"Starting training...\")\n",
    "train(model, data, optimizer, criterion, num_epochs, device)\n",
    "# Test cases\n",
    "test_cases = [\n",
    "([3, 4], ['bonjour', 'monde']), # hello world\n",
    "([5, 6, 7], ['je', 'suis', 'bien']), # i am good\n",
    "([8, 9, 10], ['comment', 'allez', 'vous']), # how are you\n",
    "([11, 12, 13], ['mon', 'nom', 'est']), # my name is\n",
    "]\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "for src, ref in test_cases:\n",
    "print(f\"\\nInput (English): {' '.join([idx_to_word_en[idx] for idx in src])}\")\n",
    "greedy_out = greedy_decode(model, src, max_len, device=device)\n",
    "beam_out = beam_search_decode(model, src, beam_width, max_len, device=device)\n",
    "topk_out = top_k_sampling_decode(model, src, top_k, max_len, device=device)\n",
    "print(f\"Reference (French): {' '.join(ref)}\")\n",
    "print(f\"Greedy: {' '.join(greedy_out)}\")\n",
    "print(f\"Beam Search: {' '.join(beam_out)}\")\n",
    "print(f\"Top-K Sampling: {' '.join(topk_out)}\")\n",
    "# BLEU scoresgreedy_bleu = evaluate_bleu(ref, greedy_out)\n",
    "beam_bleu = evaluate_bleu(ref, beam_out)\n",
    "topk_bleu = evaluate_bleu(ref, topk_out)\n",
    "print(f\"BLEU Scores - Greedy: {greedy_bleu:.3f}, Beam: {beam_bleu:.3f}, Top-K: {topk_bleu:.3f}\")\n",
    "if __name__ == \"__main__\":\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
