{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132edaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "from math import log, exp\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "import random\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')  # Required for newer NLTK versions\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "print(\"Setup completed successfully!\")\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"Class to handle text preprocessing for n-gram models\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.start_token = \"<s>\"\n",
    "        self.end_token = \"</s>\"\n",
    "        self.unk_token = \"<unk>\"\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize text\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Keep only letters, numbers, and basic punctuation\n",
    "        text = re.sub(r'[^\\w\\s.,!?;:]', '', text)\n",
    "        return text.strip()\n",
    "\n",
    "    def tokenize_sentences(self, text):\n",
    "        \"\"\"Tokenize text into sentences and words\"\"\"\n",
    "        sentences = sent_tokenize(text)\n",
    "        tokenized_sentences = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence = self.clean_text(sentence)\n",
    "            words = word_tokenize(sentence)\n",
    "            if words:  # Only add non-empty sentences\n",
    "                tokenized_sentences.append(words)\n",
    "\n",
    "        return tokenized_sentences\n",
    "\n",
    "    def add_sentence_boundaries(self, sentences, n):\n",
    "        \"\"\"Add sentence boundary tokens\"\"\"\n",
    "        processed_sentences = []\n",
    "        for sentence in sentences:\n",
    "            # Add start tokens\n",
    "            padded_sentence = [self.start_token] * (n-1) + sentence + [self.end_token]\n",
    "            processed_sentences.append(padded_sentence)\n",
    "        return processed_sentences\n",
    "\n",
    "    def handle_unknown_words(self, sentences, vocab_threshold=2):\n",
    "        \"\"\"Replace rare words with UNK token\"\"\"\n",
    "        # Count word frequencies\n",
    "        word_counts = Counter()\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                word_counts[word] += 1\n",
    "\n",
    "        # Replace rare words with UNK\n",
    "        processed_sentences = []\n",
    "        for sentence in sentences:\n",
    "            processed_sentence = []\n",
    "            for word in sentence:\n",
    "                if word_counts[word] >= vocab_threshold or word in [self.start_token, self.end_token]:\n",
    "                    processed_sentence.append(word)\n",
    "                else:\n",
    "                    processed_sentence.append(self.unk_token)\n",
    "            processed_sentences.append(processed_sentence)\n",
    "\n",
    "        return processed_sentences\n",
    "\n",
    "# Load and preprocess Brown Corpus\n",
    "print(\"Loading Brown Corpus...\")\n",
    "brown_sents = brown.sents()\n",
    "print(f\"Total sentences in Brown Corpus: {len(brown_sents)}\")\n",
    "\n",
    "# Convert to string format and preprocess\n",
    "preprocessor = TextPreprocessor()\n",
    "corpus_text = \"\"\n",
    "for sent in brown_sents[:5000]:  # Use first 5000 sentences for faster processing\n",
    "    corpus_text += \" \".join(sent) + \" \"\n",
    "\n",
    "print(\"Preprocessing corpus...\")\n",
    "tokenized_sentences = preprocessor.tokenize_sentences(corpus_text)\n",
    "print(f\"Number of processed sentences: {len(tokenized_sentences)}\")\n",
    "\n",
    "# Split into train and test sets\n",
    "train_size = int(0.8 * len(tokenized_sentences))\n",
    "train_sentences = tokenized_sentences[:train_size]\n",
    "test_sentences = tokenized_sentences[train_size:]\n",
    "\n",
    "print(f\"Training sentences: {len(train_sentences)}\")\n",
    "print(f\"Test sentences: {len(test_sentences)}\")\n",
    "\n",
    "\n",
    "class NGramModel:\n",
    "    \"\"\"N-gram language model with various smoothing techniques\"\"\"\n",
    "\n",
    "    def __init__(self, n, smoothing='laplace', alpha=1.0, discount=0.75):\n",
    "        self.n = n\n",
    "        self.smoothing = smoothing\n",
    "        self.alpha = alpha  # Laplace smoothing parameter\n",
    "        self.discount = discount  # Kneser-Ney discount parameter\n",
    "        self.ngram_counts = defaultdict(int)\n",
    "        self.context_counts = defaultdict(int)\n",
    "        self.vocabulary = set()\n",
    "        self.vocab_size = 0\n",
    "\n",
    "    def train(self, sentences):\n",
    "        \"\"\"Train the n-gram model\"\"\"\n",
    "        print(f\"Training {self.n}-gram model with {self.smoothing} smoothing...\")\n",
    "\n",
    "        # Add sentence boundaries\n",
    "        preprocessor = TextPreprocessor()\n",
    "        processed_sentences = preprocessor.add_sentence_boundaries(sentences, self.n)\n",
    "        processed_sentences = preprocessor.handle_unknown_words(processed_sentences)\n",
    "\n",
    "        # Count n-grams and contexts\n",
    "        for sentence in processed_sentences:\n",
    "            for word in sentence:\n",
    "                self.vocabulary.add(word)\n",
    "\n",
    "            for i in range(len(sentence) - self.n + 1):\n",
    "                ngram = tuple(sentence[i:i + self.n])\n",
    "                context = ngram[:-1]\n",
    "\n",
    "                self.ngram_counts[ngram] += 1\n",
    "                if self.n > 1:\n",
    "                    self.context_counts[context] += 1\n",
    "\n",
    "        self.vocab_size = len(self.vocabulary)\n",
    "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
    "        print(f\"Unique {self.n}-grams: {len(self.ngram_counts)}\")\n",
    "\n",
    "        # Special processing for Kneser-Ney smoothing\n",
    "        if self.smoothing == 'kneser_ney' and self.n > 1:\n",
    "            self._compute_kneser_ney_counts()\n",
    "\n",
    "    def _compute_kneser_ney_counts(self):\n",
    "        \"\"\"Compute continuation counts for Kneser-Ney smoothing\"\"\"\n",
    "        self.continuation_counts = defaultdict(int)\n",
    "        self.continuation_contexts = defaultdict(set)\n",
    "\n",
    "        for ngram in self.ngram_counts:\n",
    "            if len(ngram) > 1:\n",
    "                suffix = ngram[1:]\n",
    "                context = ngram[:-1]\n",
    "                self.continuation_contexts[suffix].add(context)\n",
    "\n",
    "        for suffix, contexts in self.continuation_contexts.items():\n",
    "            self.continuation_counts[suffix] = len(contexts)\n",
    "\n",
    "    def get_probability(self, ngram):\n",
    "        \"\"\"Calculate probability of an n-gram\"\"\"\n",
    "        if isinstance(ngram, str):\n",
    "            ngram = tuple(ngram.split())\n",
    "\n",
    "        if len(ngram) != self.n:\n",
    "            raise ValueError(f\"Expected {self.n}-gram, got {len(ngram)}-gram\")\n",
    "\n",
    "        if self.smoothing == 'laplace':\n",
    "            return self._laplace_probability(ngram)\n",
    "        elif self.smoothing == 'kneser_ney':\n",
    "            return self._kneser_ney_probability(ngram)\n",
    "        elif self.smoothing == 'good_turing':\n",
    "            return self._good_turing_probability(ngram)\n",
    "        else:\n",
    "            return self._mle_probability(ngram)\n",
    "\n",
    "    def _mle_probability(self, ngram):\n",
    "        \"\"\"Maximum Likelihood Estimation (no smoothing)\"\"\"\n",
    "        if self.n == 1:\n",
    "            total_count = sum(self.ngram_counts.values())\n",
    "            return self.ngram_counts[ngram] / total_count if total_count > 0 else 0\n",
    "        else:\n",
    "            context = ngram[:-1]\n",
    "            context_count = self.context_counts[context]\n",
    "            return self.ngram_counts[ngram] / context_count if context_count > 0 else 0\n",
    "\n",
    "    def _laplace_probability(self, ngram):\n",
    "        \"\"\"Laplace (Add-alpha) smoothing\"\"\"\n",
    "        if self.n == 1:\n",
    "            total_count = sum(self.ngram_counts.values())\n",
    "            return (self.ngram_counts[ngram] + self.alpha) / (total_count + self.alpha * self.vocab_size)\n",
    "        else:\n",
    "            context = ngram[:-1]\n",
    "            context_count = self.context_counts[context]\n",
    "            return (self.ngram_counts[ngram] + self.alpha) / (context_count + self.alpha * self.vocab_size)\n",
    "\n",
    "    def _kneser_ney_probability(self, ngram):\n",
    "        \"\"\"Simplified Kneser-Ney smoothing\"\"\"\n",
    "        if self.n == 1:\n",
    "            # For unigrams, use continuation counts\n",
    "            if hasattr(self, 'continuation_counts'):\n",
    "                total_continuations = sum(self.continuation_counts.values())\n",
    "                return self.continuation_counts.get(ngram, 0) / total_continuations if total_continuations > 0 else 1/self.vocab_size\n",
    "            else:\n",
    "                return self._laplace_probability(ngram)\n",
    "        else:\n",
    "            context = ngram[:-1]\n",
    "            context_count = self.context_counts[context]\n",
    "            ngram_count = self.ngram_counts[ngram]\n",
    "\n",
    "            if context_count == 0:\n",
    "                return 1 / self.vocab_size\n",
    "\n",
    "            # Discounted probability\n",
    "            discounted_count = max(ngram_count - self.discount, 0)\n",
    "            prob = discounted_count / context_count\n",
    "\n",
    "            # Add back-off probability (simplified)\n",
    "            lambda_weight = self.discount * len([ng for ng in self.ngram_counts if ng[:-1] == context]) / context_count\n",
    "            backoff_prob = self.continuation_counts.get(ngram[-1:], 0) / sum(self.continuation_counts.values()) if hasattr(self, 'continuation_counts') else 1/self.vocab_size\n",
    "\n",
    "            return prob + lambda_weight * backoff_prob\n",
    "\n",
    "    def _good_turing_probability(self, ngram):\n",
    "        \"\"\"Simplified Good-Turing smoothing\"\"\"\n",
    "        count = self.ngram_counts[ngram]\n",
    "\n",
    "        # Count of counts\n",
    "        count_counts = defaultdict(int)\n",
    "        for c in self.ngram_counts.values():\n",
    "            count_counts[c] += 1\n",
    "\n",
    "        if count == 0:\n",
    "            # Use count of singletons\n",
    "            return count_counts[1] / sum(self.ngram_counts.values()) if count_counts[1] > 0 else 1/self.vocab_size\n",
    "        else:\n",
    "            # Adjusted count using Good-Turing\n",
    "            adjusted_count = (count + 1) * count_counts[count + 1] / count_counts[count] if count_counts[count] > 0 else count\n",
    "            if self.n == 1:\n",
    "                total_count = sum(self.ngram_counts.values())\n",
    "                return adjusted_count / total_count\n",
    "            else:\n",
    "                context = ngram[:-1]\n",
    "                context_count = self.context_counts[context]\n",
    "                return adjusted_count / context_count if context_count > 0 else 0\n",
    "\n",
    "    def sentence_probability(self, sentence):\n",
    "        \"\"\"Calculate probability of a sentence\"\"\"\n",
    "        if isinstance(sentence, str):\n",
    "            sentence = sentence.lower().split()\n",
    "\n",
    "        # Add sentence boundaries\n",
    "        preprocessor = TextPreprocessor()\n",
    "        padded_sentence = [preprocessor.start_token] * (self.n-1) + sentence + [preprocessor.end_token]\n",
    "\n",
    "        log_prob = 0.0\n",
    "        for i in range(len(padded_sentence) - self.n + 1):\n",
    "            ngram = tuple(padded_sentence[i:i + self.n])\n",
    "            prob = self.get_probability(ngram)\n",
    "            if prob > 0:\n",
    "                log_prob += log(prob)\n",
    "            else:\n",
    "                log_prob += log(1e-10)  # Small probability for zero probabilities\n",
    "\n",
    "        return exp(log_prob)\n",
    "\n",
    "    def perplexity(self, test_sentences):\n",
    "        \"\"\"Calculate perplexity on test sentences\"\"\"\n",
    "        total_log_prob = 0.0\n",
    "        total_words = 0\n",
    "\n",
    "        for sentence in test_sentences:\n",
    "            if isinstance(sentence, str):\n",
    "                sentence = sentence.lower().split()\n",
    "\n",
    "            # Handle unknown words\n",
    "            processed_sentence = []\n",
    "            for word in sentence:\n",
    "                if word in self.vocabulary:\n",
    "                    processed_sentence.append(word)\n",
    "                else:\n",
    "                    processed_sentence.append(\"<unk>\")\n",
    "\n",
    "            # Add sentence boundaries\n",
    "            preprocessor = TextPreprocessor()\n",
    "            padded_sentence = [preprocessor.start_token] * (self.n-1) + processed_sentence + [preprocessor.end_token]\n",
    "\n",
    "            for i in range(len(padded_sentence) - self.n + 1):\n",
    "                ngram = tuple(padded_sentence[i:i + self.n])\n",
    "                prob = self.get_probability(ngram)\n",
    "                prob = max(prob, 1e-10)  # Avoid log(0)\n",
    "                total_log_prob += log(prob)\n",
    "                total_words += 1\n",
    "\n",
    "        avg_log_prob = total_log_prob / total_words if total_words > 0 else 0\n",
    "        return exp(-avg_log_prob)\n",
    "\n",
    "print(\"Training models with different configurations...\")\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'bigram_laplace': NGramModel(2, 'laplace', alpha=1.0),\n",
    "    'bigram_kneser_ney': NGramModel(2, 'kneser_ney', discount=0.75),\n",
    "    'trigram_laplace': NGramModel(3, 'laplace', alpha=1.0),\n",
    "    'trigram_kneser_ney': NGramModel(3, 'kneser_ney', discount=0.75),\n",
    "    'unigram_laplace': NGramModel(1, 'laplace', alpha=1.0)\n",
    "}\n",
    "\n",
    "# Train all models\n",
    "for name, model in models.items():\n",
    "    model.train(train_sentences)\n",
    "    print(f\"Trained {name}\")\n",
    "\n",
    "print(\"\\nAll models trained successfully!\")\n",
    "\n",
    "\n",
    "# Test sentences for evaluation\n",
    "test_sample_sentences = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"natural language processing is fascinating\",\n",
    "    \"machine learning algorithms are powerful\",\n",
    "    \"the quick brown fox jumps\",\n",
    "    \"artificial intelligence will change the world\"\n",
    "]\n",
    "\n",
    "print(\"Evaluating sentence probabilities...\")\n",
    "results = []\n",
    "\n",
    "for sentence in test_sample_sentences:\n",
    "    sentence_results = {'sentence': sentence}\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            prob = model.sentence_probability(sentence)\n",
    "            sentence_results[f'{name}_prob'] = prob\n",
    "        except:\n",
    "            sentence_results[f'{name}_prob'] = 0.0\n",
    "    results.append(sentence_results)\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nSentence Probability Results:\")\n",
    "print(results_df.round(10))\n",
    "\n",
    "# Calculate perplexity on test set\n",
    "print(\"\\nCalculating perplexity on test set...\")\n",
    "perplexity_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        perp = model.perplexity(test_sentences[:100])  # Use first 100 test sentences\n",
    "        perplexity_results[name] = perp\n",
    "        print(f\"{name}: {perp:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating perplexity for {name}: {e}\")\n",
    "        perplexity_results[name] = float('inf')\n",
    "\n",
    "\n",
    "# Plot perplexity comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "model_names = list(perplexity_results.keys())\n",
    "perplexities = [perplexity_results[name] for name in model_names]\n",
    "\n",
    "# Filter out infinite values for plotting\n",
    "finite_results = [(name, perp) for name, perp in zip(model_names, perplexities) if perp != float('inf')]\n",
    "if finite_results:\n",
    "    names, perps = zip(*finite_results)\n",
    "    plt.bar(names, perps)\n",
    "    plt.title('Model Perplexity Comparison')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Perplexity')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze smoothing effects\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SMOOTHING ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compare smoothing techniques\n",
    "bigram_laplace = models['bigram_laplace']\n",
    "bigram_kneser_ney = models['bigram_kneser_ney']\n",
    "\n",
    "# Test with seen and unseen n-grams\n",
    "test_ngrams = [\n",
    "    ('the', 'cat'),\n",
    "    ('cat', 'sat'),\n",
    "    ('sat', 'on'),\n",
    "    ('xyz', 'abc'),  # Unseen n-gram\n",
    "    ('<s>', 'the'),\n",
    "    ('the', '</s>')\n",
    "]\n",
    "\n",
    "print(\"Probability comparison for different n-grams:\")\n",
    "print(f\"{'N-gram':<15} {'Laplace':<12} {'Kneser-Ney':<12}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for ngram in test_ngrams:\n",
    "    laplace_prob = bigram_laplace.get_probability(ngram)\n",
    "    kn_prob = bigram_kneser_ney.get_probability(ngram)\n",
    "    print(f\"{str(ngram):<15} {laplace_prob:<12.8f} {kn_prob:<12.8f}\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Analyze why certain models perform better\n",
    "print(\"1. Vocabulary Coverage Analysis:\")\n",
    "vocabulary_stats = {}\n",
    "for name, model in models.items():\n",
    "    vocabulary_stats[name] = {\n",
    "        'vocab_size': model.vocab_size,\n",
    "        'ngram_types': len(model.ngram_counts)\n",
    "    }\n",
    "\n",
    "vocab_df = pd.DataFrame(vocabulary_stats).T\n",
    "print(vocab_df)\n",
    "\n",
    "print(\"\\n2. Sentence Length Impact on Perplexity:\")\n",
    "# Group test sentences by length and calculate perplexity\n",
    "length_groups = defaultdict(list)\n",
    "for sentence in test_sentences[:50]:\n",
    "    length_groups[len(sentence)].append(sentence)\n",
    "\n",
    "for length, sents in length_groups.items():\n",
    "    if len(sents) >= 3:  # Only analyze groups with sufficient samples\n",
    "        perp = models['bigram_laplace'].perplexity(sents)\n",
    "        print(f\"Length {length}: {perp:.2f} (n={len(sents)} sentences)\")\n",
    "\n",
    "print(\"\\n3. Impact of Smoothing Parameters:\")\n",
    "# Test different alpha values for Laplace smoothing\n",
    "alphas = [0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "alpha_perplexities = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    model = NGramModel(2, 'laplace', alpha=alpha)\n",
    "    model.train(train_sentences[:500])  # Use subset for faster training\n",
    "    perp = model.perplexity(test_sentences[:50])\n",
    "    alpha_perplexities.append(perp)\n",
    "    print(f\"Alpha = {alpha}: Perplexity = {perp:.2f}\")\n",
    "\n",
    "# Plot alpha vs perplexity\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(alphas, alpha_perplexities, 'bo-')\n",
    "plt.title('Impact of Laplace Smoothing Parameter on Perplexity')\n",
    "plt.xlabel('Alpha Value')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.grid(True)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
