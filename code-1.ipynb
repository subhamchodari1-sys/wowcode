{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "374274b3",
   "metadata": {},
   "source": [
    "# Comprehensive NLP Pipeline with spaCy and NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ab0e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.tree import Tree\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e28201e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download required NLTK data (run once)\n",
    "def download_nltk_data():\n",
    "    \"\"\"Download necessary NLTK datasets\"\"\"\n",
    "    nltk_downloads = [\n",
    "        'punkt',\n",
    "        'averaged_perceptron_tagger',\n",
    "        'averaged_perceptron_tagger_eng',\n",
    "        'maxent_ne_chunker',\n",
    "        'maxent_ne_chunker_tab',\n",
    "        'words',\n",
    "        'treebank',\n",
    "        'punkt_tab'\n",
    "    ]\n",
    "    \n",
    "    for item in nltk_downloads:\n",
    "        try:\n",
    "            nltk.download(item, quiet=True)\n",
    "            print(f\"✓ Downloaded {item}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed to download {item}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861f459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NLPPipeline:\n",
    "    \"\"\"A comprehensive NLP Pipeline class that integrates multiple NLP tasks\"\"\"\n",
    "    \n",
    "    def __init__(self, use_spacy=True, use_nltk=True):\n",
    "        self.use_spacy = use_spacy\n",
    "        self.use_nltk = use_nltk\n",
    "        \n",
    "        # Initialize spaCy model\n",
    "        if self.use_spacy:\n",
    "            try:\n",
    "                self.nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
    "                print(\"✓ spaCy model loaded successfully\")\n",
    "            except OSError:\n",
    "                print(\"✗ spaCy model 'en_core_web_sm' not found. Install with: python -m spacy download en_core_web_sm\")\n",
    "                self.use_spacy = False\n",
    "        \n",
    "        # Download NLTK data\n",
    "        if self.use_nltk:\n",
    "            download_nltk_data()\n",
    "    \n",
    "    def tokenize_text(self, text):\n",
    "        results = {}\n",
    "        if self.use_spacy:\n",
    "            doc = self.nlp_spacy(text)\n",
    "            results['spacy_tokens'] = [token.text for token in doc]\n",
    "            results['spacy_sentences'] = [sent.text.strip() for sent in doc.sents]\n",
    "        if self.use_nltk:\n",
    "            results['nltk_tokens'] = word_tokenize(text)\n",
    "            results['nltk_sentences'] = sent_tokenize(text)\n",
    "        return results\n",
    "    \n",
    "    def pos_tagging(self, text):\n",
    "        results = {}\n",
    "        if self.use_spacy:\n",
    "            doc = self.nlp_spacy(text)\n",
    "            results['spacy_pos'] = [(token.text, token.pos_, token.tag_) for token in doc]\n",
    "        if self.use_nltk:\n",
    "            tokens = word_tokenize(text)\n",
    "            results['nltk_pos'] = pos_tag(tokens)\n",
    "        return results\n",
    "    \n",
    "    def named_entity_recognition(self, text):\n",
    "        results = {}\n",
    "        if self.use_spacy:\n",
    "            doc = self.nlp_spacy(text)\n",
    "            results['spacy_entities'] = [(ent.text, ent.label_, ent.start_char, ent.end_char) for ent in doc.ents]\n",
    "        if self.use_nltk:\n",
    "            tokens = word_tokenize(text)\n",
    "            pos_tags = pos_tag(tokens)\n",
    "            chunks = ne_chunk(pos_tags, binary=False)\n",
    "            entities = []\n",
    "            for chunk in chunks:\n",
    "                if isinstance(chunk, Tree):\n",
    "                    entity_name = ' '.join([token for token, pos in chunk.leaves()])\n",
    "                    entity_type = chunk.label()\n",
    "                    entities.append((entity_name, entity_type))\n",
    "            results['nltk_entities'] = entities\n",
    "        return results\n",
    "    \n",
    "    def dependency_parsing(self, text):\n",
    "        if not self.use_spacy:\n",
    "            return None\n",
    "        doc = self.nlp_spacy(text)\n",
    "        dependencies = []\n",
    "        for token in doc:\n",
    "            dependencies.append({\n",
    "                'text': token.text,\n",
    "                'dep': token.dep_,\n",
    "                'head': token.head.text,\n",
    "                'children': [child.text for child in token.children]\n",
    "            })\n",
    "        return dependencies\n",
    "    \n",
    "    def lemmatization(self, text):\n",
    "        if not self.use_spacy:\n",
    "            return None\n",
    "        doc = self.nlp_spacy(text)\n",
    "        return [(token.text, token.lemma_) for token in doc]\n",
    "    \n",
    "    def full_pipeline(self, text):\n",
    "        return {\n",
    "            'original_text': text,\n",
    "            'tokenization': self.tokenize_text(text),\n",
    "            'pos_tagging': self.pos_tagging(text),\n",
    "            'ner': self.named_entity_recognition(text),\n",
    "            'dependency_parsing': self.dependency_parsing(text),\n",
    "            'lemmatization': self.lemmatization(text)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6152cf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display_results(results):\n",
    "    print(f\"\\n📝 ORIGINAL TEXT:\")\n",
    "    print(f\"{results['original_text']}\\n\")\n",
    "    \n",
    "    print(\"🔤 TOKENIZATION RESULTS:\")\n",
    "    tokenization = results['tokenization']\n",
    "    if 'spacy_tokens' in tokenization:\n",
    "        print(f\"spaCy Tokens ({len(tokenization['spacy_tokens'])}): {tokenization['spacy_tokens'][:10]}...\")\n",
    "        print(f\"spaCy Sentences: {len(tokenization['spacy_sentences'])}\")\n",
    "    if 'nltk_tokens' in tokenization:\n",
    "        print(f\"NLTK Tokens ({len(tokenization['nltk_tokens'])}): {tokenization['nltk_tokens'][:10]}...\")\n",
    "        print(f\"NLTK Sentences: {len(tokenization['nltk_sentences'])}\")\n",
    "    \n",
    "    print(f\"\\n🏷️  POS TAGGING RESULTS:\")\n",
    "    pos_results = results['pos_tagging']\n",
    "    if 'spacy_pos' in pos_results:\n",
    "        print(\"spaCy POS Tags (first 10):\")\n",
    "        for word, pos, tag in pos_results['spacy_pos'][:10]:\n",
    "            print(f\"  {word:15} -> {pos:8} ({tag})\")\n",
    "    if 'nltk_pos' in pos_results:\n",
    "        print(\"NLTK POS Tags (first 10):\")\n",
    "        for word, tag in pos_results['nltk_pos'][:10]:\n",
    "            print(f\"  {word:15} -> {tag}\")\n",
    "    \n",
    "    print(f\"\\n🎯 NAMED ENTITY RECOGNITION:\")\n",
    "    ner_results = results['ner']\n",
    "    if 'spacy_entities' in ner_results and ner_results['spacy_entities']:\n",
    "        for entity, label, start, end in ner_results['spacy_entities']:\n",
    "            print(f\"  {entity:20} -> {label}\")\n",
    "    if 'nltk_entities' in ner_results and ner_results['nltk_entities']:\n",
    "        for entity, label in ner_results['nltk_entities']:\n",
    "            print(f\"  {entity:20} -> {label}\")\n",
    "    \n",
    "    if results['dependency_parsing']:\n",
    "        print(f\"\\n🌳 DEPENDENCY PARSING (first 5 tokens):\")\n",
    "        for dep in results['dependency_parsing'][:5]:\n",
    "            print(f\"  {dep['text']:15} -> {dep['dep']:10} (head: {dep['head']})\")\n",
    "    \n",
    "    if results['lemmatization']:\n",
    "        print(f\"\\n📖 LEMMATIZATION (first 10 tokens):\")\n",
    "        for original, lemma in results['lemmatization'][:10]:\n",
    "            if original != lemma:\n",
    "                print(f\"  {original:15} -> {lemma}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72da2375",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_sample_texts():\n",
    "    pipeline = NLPPipeline()\n",
    "    sample_texts = [\n",
    "        \"Apple Inc. is planning to open a new store in New York City next month. CEO Tim Cook will attend the opening ceremony.\",\n",
    "        \"The quick brown fox jumps over the lazy dog. This sentence contains every letter of the alphabet.\",\n",
    "        \"Barack Obama was the 44th President of the United States. He served from 2009 to 2017 and was born in Hawaii.\"\n",
    "    ]\n",
    "    try:\n",
    "        treebank_sample = ' '.join(treebank.words()[:50])\n",
    "        sample_texts.append(treebank_sample)\n",
    "        print(\"✓ Added Treebank sample\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Could not load Treebank sample: {e}\")\n",
    "    for text in sample_texts:\n",
    "        results = pipeline.full_pipeline(text)\n",
    "        display_results(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7f8a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_libraries():\n",
    "    pipeline = NLPPipeline()\n",
    "    test_text = \"Dr. John Smith from Harvard University published a paper about machine learning in Nature magazine.\"\n",
    "    results = pipeline.full_pipeline(test_text)\n",
    "    print(\"📊 TOKENIZATION COMPARISON:\")\n",
    "    print(\"spaCy:\", results['tokenization'].get('spacy_tokens'))\n",
    "    print(\"NLTK:\", results['tokenization'].get('nltk_tokens'))\n",
    "    print(\"\\n🎯 NER COMPARISON:\")\n",
    "    print(\"spaCy:\", results['ner'].get('spacy_entities'))\n",
    "    print(\"NLTK:\", results['ner'].get('nltk_entities'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e94a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_pos_distribution_chart(results):\n",
    "    pos_results = results['pos_tagging']\n",
    "    if 'spacy_pos' in pos_results:\n",
    "        pos_counts = defaultdict(int)\n",
    "        for _, pos, _ in pos_results['spacy_pos']:\n",
    "            pos_counts[pos] += 1\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.bar(pos_counts.keys(), pos_counts.values(), color='skyblue', edgecolor='navy')\n",
    "        plt.title('POS Tag Distribution (spaCy)')\n",
    "        plt.xlabel('POS Tags')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9557f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_entities():\n",
    "    try:\n",
    "        import spacy\n",
    "        from spacy import displacy\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        text = \"Apple Inc. was founded by Steve Jobs in Cupertino, California.\"\n",
    "        doc = nlp(text)\n",
    "        html = displacy.render(doc, style=\"ent\", jupyter=False)\n",
    "        with open(\"entity_visualization.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(html)\n",
    "        print(\"✓ Entity visualization saved to 'entity_visualization.html'\")\n",
    "    except Exception as e:\n",
    "        print(\"Could not visualize entities:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae08e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"🚀 Starting NLP Pipeline Demonstration\")\n",
    "analyze_sample_texts()\n",
    "compare_libraries()\n",
    "pipeline = NLPPipeline()\n",
    "sample_text = \"Natural language processing enables interaction between computers and human language.\"\n",
    "results = pipeline.full_pipeline(sample_text)\n",
    "create_pos_distribution_chart(results)\n",
    "visualize_entities()\n",
    "print(\"✅ NLP Pipeline Analysis Complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
