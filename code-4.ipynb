{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "import re\n",
        "from datasets import load_dataset  # Requires 'datasets' library\n",
        "from sklearn.decomposition import PCA  # Optional, for minimal visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load IMDb dataset (subset for efficiency)\n",
        "def load_imdb_data(max_samples=1000):\n",
        "    dataset = load_dataset(\"imdb\", split=\"train[:1000]\")  # Use train split, first 1000 samples\n",
        "    texts = dataset[\"text\"][:max_samples]\n",
        "    labels = dataset[\"label\"][:max_samples]  # 0: negative, 1: positive\n",
        "    # Split into train/test (80-20)\n",
        "    train_size = int(0.8 * len(texts))\n",
        "    train_texts = texts[:train_size]\n",
        "    test_texts = texts[train_size:]\n",
        "    train_labels = labels[:train_size]\n",
        "    test_labels = labels[train_size:]\n",
        "    return train_texts, train_labels, test_texts, test_labels\n",
        "\n",
        "# Tokenization functions\n",
        "def tokenize_whitespace(texts):\n",
        "    tokenized = []\n",
        "    for text in texts:\n",
        "        cleaned = re.sub(r'[^\\w\\s]', '', text.lower())\n",
        "        tokens = cleaned.split()\n",
        "        tokenized.append(tokens[:100])  # Limit tokens per review for efficiency\n",
        "    return tokenized\n",
        "\n",
        "def tokenize_char(texts):\n",
        "    tokenized = []\n",
        "    for text in texts:\n",
        "        chars = [c.lower() for c in text if c.isalpha()][:100]  # Limit for efficiency\n",
        "        tokenized.append(chars)\n",
        "    return tokenized\n",
        "\n",
        "def train_bpe(texts, num_merges=10):\n",
        "    cleaned_texts = [re.sub(r'[^\\w\\s]', '', text.lower()) for text in texts]\n",
        "    all_text = ' '.join(cleaned_texts)\n",
        "    words = [w for w in all_text.split() if w]\n",
        "    vocab = Counter(words)\n",
        "    splits = {word: list(word) for word in vocab if word}\n",
        "\n",
        "    def get_pairs(splits):\n",
        "        pairs = defaultdict(int)\n",
        "        for word, freq in vocab.items():\n",
        "            symbols = splits[word]\n",
        "            for i in range(len(symbols) - 1):\n",
        "                pairs[(symbols[i], symbols[i + 1])] += freq\n",
        "        return pairs\n",
        "\n",
        "    bpe_merges = []\n",
        "    for _ in range(num_merges):\n",
        "        pairs = get_pairs(splits)\n",
        "        if not pairs:\n",
        "            break\n",
        "        bigram = max(pairs, key=pairs.get)\n",
        "        bpe_merges.append(bigram)\n",
        "        new_splits = {}\n",
        "        for word, symbols in splits.items():\n",
        "            new_symbols = []\n",
        "            i = 0\n",
        "            while i < len(symbols):\n",
        "                if i + 1 < len(symbols) and symbols[i] == bigram[0] and symbols[i + 1] == bigram[1]:\n",
        "                    new_symbol = bigram[0] + bigram[1]\n",
        "                    new_symbols.append(new_symbol)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_symbols.append(symbols[i])\n",
        "                    i += 1\n",
        "            new_splits[word] = new_symbols\n",
        "        splits = new_splits\n",
        "\n",
        "    def apply_bpe(word):\n",
        "        symbols = list(word)\n",
        "        for bigram in bpe_merges:\n",
        "            i = 0\n",
        "            while i < len(symbols) - 1:\n",
        "                if symbols[i] == bigram[0] and symbols[i + 1] == bigram[1]:\n",
        "                    symbols[i] = bigram[0] + bigram[1]\n",
        "                    del symbols[i + 1]\n",
        "                else:\n",
        "                    i += 1\n",
        "        return symbols if symbols else [word]\n",
        "\n",
        "    def tokenize(texts):\n",
        "        tokenized = []\n",
        "        for text in texts:\n",
        "            cleaned = re.sub(r'[^\\w\\s]', '', text.lower())\n",
        "            words = cleaned.split()\n",
        "            tok_sent = []\n",
        "            for word in words[:100]:  # Limit for efficiency\n",
        "                if word:\n",
        "                    tok_sent.extend(apply_bpe(word))\n",
        "            tokenized.append(tok_sent)\n",
        "        return tokenized\n",
        "\n",
        "    return tokenize\n",
        "\n",
        "# Build vocabulary\n",
        "def build_vocab(tokenized_corpus):\n",
        "    flat_tokens = [tok for sent in tokenized_corpus for tok in sent]\n",
        "    vocab = list(set(flat_tokens))\n",
        "    token_to_id = {tok: i for i, tok in enumerate(vocab)}\n",
        "    id_to_token = {i: tok for tok, i in token_to_id.items()}\n",
        "    return token_to_id, id_to_token, len(vocab)\n",
        "\n",
        "# Generate training data for Word2Vec\n",
        "def generate_training_data(tokenized_corpus, token_to_id, window_size=2):\n",
        "    training_data = []\n",
        "    for sent in tokenized_corpus:\n",
        "        sent_ids = [token_to_id[tok] for tok in sent if tok in token_to_id]\n",
        "        for i in range(len(sent_ids)):\n",
        "            for j in range(max(0, i - window_size), min(len(sent_ids), i + window_size + 1)):\n",
        "                if i != j:\n",
        "                    training_data.append((sent_ids[i], sent_ids[j]))\n",
        "    return training_data[:5000]  # Limit for efficiency\n",
        "\n",
        "# Skip-gram Word2Vec model\n",
        "class SkipGram(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.in_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.out_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "    def forward(self, center_ids):\n",
        "        centers_emb = self.in_embeddings(center_ids)\n",
        "        out_emb = self.out_embeddings.weight\n",
        "        scores = torch.matmul(centers_emb, out_emb.t())\n",
        "        return scores\n",
        "\n",
        "# Train Word2Vec\n",
        "def train_word2vec(training_data, vocab_size, embed_dim=50, epochs=10, lr=0.001):\n",
        "    model = SkipGram(vocab_size, embed_dim)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for center, context in training_data:\n",
        "            center_t = torch.tensor([center], dtype=torch.long)\n",
        "            context_t = torch.tensor([context], dtype=torch.long)\n",
        "            optimizer.zero_grad()\n",
        "            scores = model(center_t)\n",
        "            loss = criterion(scores, context_t)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f'Word2Vec Epoch {epoch}, Average loss: {total_loss / len(training_data):.4f}')\n",
        "    return model.in_embeddings.weight.data.numpy()\n",
        "\n",
        "# GloVe simple implementation\n",
        "def train_glove(tokenized_corpus, token_to_id, embed_dim=50, epochs=10, lr=0.01):\n",
        "    vocab_size = len(token_to_id)\n",
        "    cooc = np.zeros((vocab_size, vocab_size))\n",
        "    window = 2\n",
        "    for sent in tokenized_corpus:\n",
        "        sent_ids = [token_to_id[tok] for tok in sent if tok in token_to_id]\n",
        "        for i in range(len(sent_ids)):\n",
        "            for j in range(max(0, i - window), min(len(sent_ids), i + window + 1)):\n",
        "                if i != j:\n",
        "                    cooc[sent_ids[i], sent_ids[j]] += 1.0 / abs(i - j)\n",
        "    W = np.random.normal(0, 0.01 / embed_dim, (vocab_size, embed_dim))\n",
        "    Wtilde = np.random.normal(0, 0.01 / embed_dim, (vocab_size, embed_dim))\n",
        "    b = np.zeros(vocab_size)\n",
        "    btilde = np.zeros(vocab_size)\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for i in range(vocab_size):\n",
        "            for j in range(vocab_size):\n",
        "                if cooc[i, j] > 0:\n",
        "                    inner = np.dot(W[i], Wtilde[j]) + b[i] + btilde[j]\n",
        "                    target = np.log(max(cooc[i, j], 1e-10))\n",
        "                    error = inner - target\n",
        "                    weight = 1.0\n",
        "                    total_loss += weight * error ** 2\n",
        "                    dWi = weight * error * Wtilde[j]\n",
        "                    dWtildej = weight * error * W[i]\n",
        "                    dbi = weight * error\n",
        "                    dbtildej = weight * error\n",
        "                    W[i] -= lr * dWi\n",
        "                    Wtilde[j] -= lr * dWtildej\n",
        "                    b[i] -= lr * dbi\n",
        "                    btilde[j] -= lr * dbtildej\n",
        "        print(f'GloVe Epoch {epoch}, Loss: {total_loss:.4f}')\n",
        "    embeddings = (W + Wtilde) / 2.0\n",
        "    return embeddings\n",
        "\n",
        "# Word relationship analysis (cosine similarity)\n",
        "def print_word_relationship(embeddings, token_to_id):\n",
        "    words = ['good', 'bad', 'movie', 'film']\n",
        "    for word1 in words:\n",
        "        for word2 in words:\n",
        "            if word1 != word2 and word1 in token_to_id and word2 in token_to_id:\n",
        "                id1 = token_to_id[word1]\n",
        "                id2 = token_to_id[word2]\n",
        "                emb1 = embeddings[id1]\n",
        "                emb2 = embeddings[id2]\n",
        "                sim = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
        "                print(f\"Cosine similarity between '{word1}' and '{word2}': {sim:.4f}\")\n",
        "\n",
        "# Minimal visualization (optional, comment out if sklearn not available)\n",
        "def visualize_embeddings(embeddings, id_to_token, method='PCA'):\n",
        "    words = ['good', 'bad', 'movie', 'film', 'great', 'poor']\n",
        "    indices = [id_to_token[i] for i in range(len(id_to_token)) if id_to_token[i] in words]\n",
        "    if not indices:\n",
        "        print(\"No target words found for visualization.\")\n",
        "        return\n",
        "    idxs = [token_to_id[word] for word in indices]\n",
        "    emb_subset = embeddings[idxs]\n",
        "    pca = PCA(n_components=2)\n",
        "    emb_2d = pca.fit_transform(emb_subset)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    for i, word in enumerate(indices):\n",
        "        plt.scatter(emb_2d[i, 0], emb_2d[i, 1])\n",
        "        plt.annotate(word, (emb_2d[i, 0], emb_2d[i, 1]))\n",
        "    plt.title('Embedding Visualization (PCA)')\n",
        "    plt.show()\n",
        "\n",
        "# Sentence embedding\n",
        "def get_sentence_embedding(sentence_tokens, token_to_id, embeddings):\n",
        "    valid_tokens = [tok for tok in sentence_tokens if tok in token_to_id]\n",
        "    if not valid_tokens:\n",
        "        return np.zeros(embeddings.shape[1])\n",
        "    ids = [token_to_id[tok] for tok in valid_tokens]\n",
        "    emb = embeddings[ids]\n",
        "    return np.mean(emb, axis=0)\n",
        "\n",
        "# Simple classifier for downstream task\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sigmoid(self.linear(x))\n",
        "\n",
        "def train_classifier(train_reps, train_labels, embed_dim, epochs=20):\n",
        "    model = Classifier(embed_dim)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "    train_inputs = torch.tensor(train_reps, dtype=torch.float32)\n",
        "    train_targets = torch.tensor(train_labels, dtype=torch.float32).unsqueeze(1)\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(train_inputs)\n",
        "        loss = criterion(outputs, train_targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return model\n",
        "\n",
        "def evaluate_classifier(model, test_reps, test_labels):\n",
        "    test_inputs = torch.tensor(test_reps, dtype=torch.float32)\n",
        "    with torch.no_grad():\n",
        "        preds = model(test_inputs).round()\n",
        "    acc = (preds == torch.tensor(test_labels).unsqueeze(1).float()).float().mean().item()\n",
        "    return acc\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    train_texts, train_labels, test_texts, test_labels = load_imdb_data(max_samples=1000)\n",
        "    corpus = train_texts  # Use training texts for embedding training\n",
        "\n",
        "    # Define tokenizers\n",
        "    tokenizers = {\n",
        "        'Whitespace': tokenize_whitespace,\n",
        "        'Character': tokenize_char,\n",
        "        'BPE': train_bpe(corpus, num_merges=10)\n",
        "    }\n",
        "\n",
        "    embed_methods = ['Word2Vec', 'GloVe']\n",
        "    results = {}\n",
        "\n",
        "    for tok_name, tokenizer in tokenizers.items():\n",
        "        print(f\"\\n--- {tok_name} Tokenization ---\")\n",
        "        tokenized_corpus = tokenizer(corpus)\n",
        "        token_to_id, id_to_token, vocab_size = build_vocab(tokenized_corpus)\n",
        "        training_data = generate_training_data(tokenized_corpus, token_to_id)\n",
        "        print(f\"Vocab size: {vocab_size}, Training pairs: {len(training_data)}\")\n",
        "\n",
        "        tokenized_train = tokenizer(train_texts)\n",
        "        tokenized_test = tokenizer(test_texts)\n",
        "\n",
        "        method_results = {}\n",
        "        for method in embed_methods:\n",
        "            print(f\"\\nTraining {method}...\")\n",
        "            if method == 'Word2Vec':\n",
        "                embeddings = train_word2vec(training_data, vocab_size, embed_dim=50)\n",
        "            else:\n",
        "                embeddings = train_glove(tokenized_corpus, token_to_id, embed_dim=50)\n",
        "\n",
        "            # Word relationship analysis\n",
        "            print(f\"\\n{method} Word Relationships:\")\n",
        "            print_word_relationship(embeddings, token_to_id)\n",
        "\n",
        "            # Optional visualization (uncomment to use)\n",
        "            # visualize_embeddings(embeddings, id_to_token)\n",
        "\n",
        "            # Downstream task: sentiment classification\n",
        "            train_reps = np.array([get_sentence_embedding(sent, token_to_id, embeddings) for sent in tokenized_train])\n",
        "            test_reps = np.array([get_sentence_embedding(sent, token_to_id, embeddings) for sent in tokenized_test])\n",
        "\n",
        "            classifier = train_classifier(train_reps, train_labels, embeddings.shape[1])\n",
        "            acc = evaluate_classifier(classifier, test_reps, test_labels)\n",
        "            method_results[method] = acc\n",
        "            print(f\"{method} downstream accuracy: {acc:.4f}\")\n",
        "\n",
        "        results[tok_name] = method_results\n",
        "\n",
        "    # Comparative analysis\n",
        "    print(\"\\nComparative Analysis Summary (Accuracy on Sentiment Classification):\")\n",
        "    for tok, methods in results.items():\n",
        "        print(f\"{tok}: Word2Vec = {methods['Word2Vec']:.4f}, GloVe = {methods['GloVe']:.4f}\")\n",
        "\n",
        "    print(\"\\nAnalysis:\")\n",
        "    print(\"- Whitespace: Captures whole words, effective for common terms but struggles with OOV.\")\n",
        "    print(\"- Character: Fine-grained, handles OOV well but may lose semantic context.\")\n",
        "    print(\"- BPE: Balances word and subword, better for rare words and morphological variations.\")\n",
        "    print(\"Higher accuracy suggests better representations for sentiment classification.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}